The objective of this paper is to clarify my ideas about a proposal of Jonathan Lee regarding the anthropic consequences of the Fermi paradox in  the AGI field. Though I do not deem it a key element for solving the AI safety problem, I think it is one that must be considered before a seed AI is created.

## AGI variation of the Fermi paradox

As a quick introduction, Fermi paradox states that "Where is everybody?", referring to other civilizations able to make interstellar travel and/or communicate through space. For the purposes of this essay, we well rephrase it however as follows: "Where are the AGI's?".

The same arguments regarding the Fermi paradox apply to this variation, and, even taking into account that a superintelligence may have access to an insight that resolves the paradox, there is a nontrivial chance that the FAI shares our doubts. Breaking down the problem to fundamentals, suppose a FAI is created. Then it could suspect that there are other AGI's in the Universe whose light cones have not yet intersected with its own. The prior for this seems reasonably high taking anthropics into account: the existence of a FAI is a definite proof of the possibility of AGIs, and as far as we know we have no reason to belief that there is a local condition in the Solar system that makes the development of AGI possible here but impossible somewhere else.

Particularly worrying, from the FAI point of view, and our own if we got the alignment part right, is the possibility that this foreign AGI is optimizing for a fairly different set of values, say tiling the universe with paperclips. Furthermore, we cannot dismiss the chance that this foreign AGI is powerful enough so as for negotiation to not be a possibility.

This paranoid belief might result in a complicated reaction, where the FAI would try to optimize agressively to push our technology level to the point where it could start propagating mankind along possibly directions as to escape this imaginary threat, maybe in a way hardly perceived as friendly. It could for example forcibly upload us to servers that would be scattered by the way of Neumann probes along the galaxy. I want to remark that this may not be undesired behavior.

The desperate runaway would continue until the FAI manages to propagate beyond the cosmological horizon, when its cradle, located at the center of its sphere of influence, would be deemed safe; no malevolent AGI could from this moment and on reach planet Earth without breaking the laws of physics. Once this safety threshold is reached, the FAI will hopefully start optimizing for human values. As time goes on, the amplitude of the safe zone will gradually increase.

I suggest it is important to ponder about what would happen to the instances of the FAI (presumably carrying humans with them to save them from the imaginary enemy) that are about to cross the cosmological horizon. It seems reasonable that the runaway for those unfortunates in the peripheral zone will never end, as they will never be able to assure the nonexistence of a malevolent AGI coming from the direction opposite to mankind's cradle. If the FAI places a specially high value in the conservation of individual human life, it may opt for evacuating all the unsafe area, throwing the whole of mankind into the aforementioned perpetual run.

It is believed that the universe has a finite length, so the FAI may assign lower and lower probabilities to the existence of other malevolent AGI's as it explores more of the reachable space. This could lower the expected utility associated with running away enough as to eventually stop the runaway.

It might also be the case that once the FAI reaches the cosmological horizon respect to Earth it will decide to turn back towards a inhabitable planet visited that it can still reach (Earth would by then be beyond its cosmological horizon by then).

##Proposal
