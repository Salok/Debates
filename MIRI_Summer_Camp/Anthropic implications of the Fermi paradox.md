The objective of this paper is to clarify my ideas about a proposal of Jonathan Lee regarding the anthropic consequences of the Fermi paradox in the AGI field. Though I do not deem it a key element for solving the AI safety problem, I think it is one that must be considered before a seed AI is created.

## When you change Friendly for Fermi

As a quick introduction, Fermi paradox states that "Where is everybody?", referring to the apparent contraction between the possibility of interstellar travel and/or communication through space and the lack of signals of other civilizations. For the purposes of this essay, we well rephrase it however as follows: "Where are the AGI's?".

The same arguments regarding the Fermi paradox apply to this variation, and, even taking into account that a superintelligence may have access to an insight that resolves the paradox, there is a nontrivial chance that the FAI shares our doubts. Breaking down the problem to fundamentals, suppose a FAI is created. Then it could suspect that there are other AGI's in the Universe whose light cones have not yet intersected with its own. The prior for this seems reasonably high taking anthropics into account: the existence of a FAI is a definite proof of the possibility of AGIs, and as far as we know we have no reason to belief that there is a local condition in the Solar system that makes the development of AGI possible here but impossible somewhere else.

Particularly worrying, from the FAI point of view, and our own if we got the alignment part right, is the possibility that this foreign AGI is optimizing for a fairly different set of values, say tiling the universe with paperclips. Furthermore, we cannot dismiss the chance that this alien AGI is powerful enough so as to dismiss every FAI's attempt of negotiation.

This paranoid belief might result in a complicated reaction, where the FAI would try to optimize aggressively to push our technology level to the point where it could start propagating mankind along possibly directions as to escape this imaginary threat, maybe in a way hardly perceived as friendly. It could for example forcibly upload us to servers that would be scattered by the way of Neumann probes along the galaxy. I want to remark that this may not be undesired behavior.

The desperate runaway would continue until the FAI manages to propagate beyond the cosmological horizon, when its cradle, located at the center of its sphere of influence, would be deemed safe; no malevolent AGI could from this moment and on reach planet Earth without breaking the laws of physics (if our understanding of cosmological expansion is indeed correct). Once this safety threshold is reached, the FAI will hopefully start optimizing for human values. As time goes on, the amplitude of the safe zone will gradually increase.

I suggest it is important to ponder about what would happen to the instances of the FAI (presumably carrying humans with them to save them from the imaginary enemy) that are about to cross the cosmological horizon. It seems reasonable that the runaway for those unfortunates in the peripheral zone will never end, as they will never be able to assure the nonexistence of a malevolent AGI coming from the direction opposite to mankind's cradle. If the FAI places a specially high value in the conservation of individual human life, it may opt for evacuating all the unsafe area, throwing the whole of mankind into the aforementioned perpetual run.

It is believed that the universe has a finite length, so the FAI may assign lower and lower probabilities to the existence of other malevolent AGI's as it explores more of the reachable space. This could lower the expected utility associated with running away enough as to eventually stop the runaway. It might also be the case that once the FAI reaches the cosmological horizon respect to Earth it will decide to turn back towards a inhabitable planet visited that it can still reach (Earth would by then be beyond its relative cosmological horizon by then).

Jonathan guesses that our strongest bet is to assume the FAI will stop once it has colonized the local group, as it is reasonably to assign a near zero probability of finding anything between the void that isolates local groups.

I however still worry about the possibility of a UFAI probe launched from which it is now other causally disconnected local group, but said probe has traveled far enough to be still within the accessible zone of an instance of the FAI in the border of our local group.

It is been also proposed the possibility of a malevolent probe waiting until the place it is located is declared a safe zone. However, this scenario is very unlikely. A sufficiently powerful AI without moral constraints would not wait until the FAI has grown strong enough to pose a challenge. It will devour it remorselessly. As for the case of said probe being a newly formed AI, I strongly suspect developing a UFAI when a FAI is already in the process of conquering the local group is borderline impossible.

##Proposals

It is unclear whether we want this "failure mode" to be fixed or not. Patching the FAI programming to prevent this scenario from happening may well be akin to condemning human civilization to extinction, once the alien paperclip maximizer enters our light cone.

Supposing we do not want to alter this behavior, testing would become even more difficult. The initial actions taken by the FAI may well resemble those that we would typically expect from a UFAI, prompting us to reject valid desings of FAI; and conversely allowing us to accept UFAI's as valid solutions if we become too lenient in our testing metrics. I would argue this is one point in favor of building a "math oracle" to validate the designs of FAI's instead of relying in conventional empiric methods.

On the other hand, if we wanted to prevent this from ever happening, some hacks could be applied. To the extent of my knowledge, hardcoding a low prior (very close to zero, or even zero) for alien AIs in the seed AI world model would prevent the paranoid behavior from ever triggering.

With no further addend, I end this essay compelling the reader to comment and discuss everything prompted in their minds. Also, it would be much appreciated if someone could provide some graphs about light cones intersecting to illustrate the text.
