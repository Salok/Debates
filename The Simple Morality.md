#The Simple Morality

We constantly obsess over the nature of morality. We pretty much nail morality when working on object level, as evidenced by the sheer amount of moral theories who end up reaching the same conclusions, yet out meta-theories, our search for the source of morality, diverge wildly, suggesting most of us believe in a wrong explanation. It is my intention to restore a naive vision of morality, a la Simple Truth. The main reason to write this is to expose my own ideas to myself, and see whether any of them are valuable, so that I can rebuild my beliefs upon them. And, of course, to find the flaws and annihilate them, cleansing my mind under the fire of reason. Ultimately, my goal in mind is an analysis of the plausibility of the CEV concept.

The first grave mistake people make when talking about morality is not specifying a frame from which to discuss morality. There is no intrinsic property called rightness in the universe, so no action can be "right" or "wrong" if there is no agent which reasons about such properties. Now, this suggests that morality depends upon the preferences of the agent. I'll go one step further and say that what makes us talk about morality are truly our preferences.

Philosophers through history have made a sharp distinction between what we want and what we want to want. They have called the former thing our desires and the second thing moral compass or similar. I find such a division disturbing and unnatural. We do not need two buckets for those two things. Both of them are preferences, which in ultimate instance determine our acts. We need a distinction however between our true preferences, and our stated preferences, which is our model of what our preferences are.

What is a preference? This question seems resolved when we picture rational agents as optimization processes in the physical universe. Surely there must be a utility function attached to each agent, that effectively determines its actions. By preferences (and thus morality) I do refer to the formal specification of our utility function.

Other sensible problem would arise from the fact that preferences seem to change in relation to information available to us. Surely, the FAI would have to use its tremendous databases when extracting the CEV of an individual. A cheap way out of this is to assume that a preference subject to change in light of new information was actually an instrumental preference, being the final preference something other.

##The problem of modeling morality

One apparently unsurmountable difficulty when dealing with our preferences is our lack of introspection. We do not seem able to pinpoint the exact specification that evolution forged into us; we are unable to read our own code. We do know some algorhythm must explain our behavior, but we are still to young to comprehend it. Our experience with morality is akin to a black box, from which certain imperatives arise. So we have to rely on induction, and so will have a FAI implementing individual CEV, to try guessing the most simple set of rules that govern a subject's morality. This task has proven to be daunting due to several reasons, related to the inherent difficulty of modeling a real world process and to the content of our preferences.

Regarding content, we seem to have contradictory preferences. There is a precise mathematical description of a rational set of preferences, [VNM rationality] (https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem), and several psychological studies conclude that ours do not satisfy the required axioms. For example, we will accept a bet previously rejected when framed in a slightly different way. This could be consequence of our inability to fully compute our utility function (in other words, due to the effect of heuristics). Are truly contradictory preferences logically possible or just the apparent product of heuristics? I have trouble picturing an agent with contradictory preferences. However, such a thing seems possible if heuristics mediate between our stated preferences and our actions. Let me explain myself.

We could imagine contradictory preferences as two statements which cannot be fulfilled at the same time. Two preferences such as: "I like apples" and "I don't like apples". But this kind of contradiction can only exist at the indirect specification level. At object level, one will prevail. Either the agent will eat apples or he won't. The inconsistency is in the map, not in the territory.

Another difficulty when trying to model preferences relates to meta-preferences. We can have preferences of second order, which refer to which preferences we would like to have. For example, a drug addict may have a first order preference to take drugs and a second order preference not to do so. He could even have third order preferences and so forth. This complicates the analysis.

Other problem to take into account is that preferences evolve with time. Internally, as meta-preferences are realized they change the content of lesser order preferences. This change is gradual as the computation and implementation of preferences is subject to the imperfect reality of our inefficient bodies. Furthermore, for a meta preference to take effect its associated utility must surpass the utility provided by the lesser order preferences to be changed. Returning to the drug addict example, it may be impossible for him to ever leave drugs if the utility received from taking the drug has already surpassed the utility of changing his base preference.

Our preferences can change due to extern stimuli as well. We may read a book and be inspired by its charismatic protagonist, changing our future reactions. This implies that a FAI implementing individual CEV must either make a model of how the subject preferences will evolve over time in his daily life, or periodically reevaluate his model of preferences of the subject. A compromise between both options seems like a good `a priori` template solution; the FAI will be able to rely in the evolving model between each reevaluation, while each update will make sure the model does not depart too much from reality.

Regarding the problems arising from meta preferences, I dare to suggest a method of analysis. The FAI would rely on scientific induction and Bayesian prior reasoning to deduce a reasonable set of preferences. Those will be arranged accorded to order, and then every higher order preference will be applied to optimize the base preferences until we are left with only coherent base preferences. In other words, we will first look the highest order preferences, say order n, and optimize n-1 order preferences according to those preferences. The n-1 optimized preferences will then be used to optimize n-2 preferences, and so forth, until first order preferences are reached.

I have been asked how could we infer higher order preferences from mere observation. My mind turns again to scientific induction and Occam prior reasoning. We could observe how a subject changes its preferences over time, and infer a second-order preference, instead of postulating several conditional or partial preferences. For example, if we observe that a subject gradually starts implementing preferences with content such as doing exercise and eating healthy, we could infer the existence of a second order preference "I want preferences that lead me to a healthier life". We can note that those higher order preferences are only relevant while they are not fully implemented in base level. Once our subject has implemented every first-order preference on health, the second order preference becomes irrelevant in the creation of new preferences, so it becomes irrelevant. It would then become a first order preference, absorbing every preference that it could have generated while being second order. So "I want to do exercise" and "I want to eat healthy" are grouped under the general preference "I want to do things that lead me to a healthier life" while "I want preferences that lead me to a healthier life" would become redundant. This is fully consistent with the generally accepted Kolgomorov simplicity epistemic axiom.

It is unclear whether inconsistencies could arise from the optimization process, though it would seem like an inconsistent system is symptom of a poor construction of the initial set of the rules abstracting the utility function of the subject, as with our previous example over the apples. Another problem arises when we allow appliance of induction over preferences of increasing order. There is a very good chance of us having a meta-preference stating "I want to preserve my first order preferences". Similarly, a third order preference stating "I want to preserve my second order preferences" may be possible, and so forth. A FAI observing those preferences may conclude by induction a infinite order preference of preservation of preferences. The method would be rendered useless then. Note however that the example provided would make for an inconsistent set of preferences may there be another meta preference; this other hypothetical meta-preference would require changing a base preference to be optimized, but that contradicts the preservation preference. We are tempted to conclude that such an infinite order preference would be fruit again of a stupid epistemic system, but it is difficult to say whether other similar in structure preferences are possible.

##Extrapolating morality from a set of individuals

So far we have examined how a morality may be extracted from an individual. Now we turn our attention to the far more difficult problem of extrapolating the CEV of a community.

Our previous reflections on preferences suggest a certain arbitrariness surrounding the contents of those. We have to realize however that there are some constraints, the most important of which is of evolutionary nature. There are certain preferences inconsistent with the mere existence of the individual. For example, an individual wanting to terminate its own life will soon cease to exist. It may be possible to admit that such a preference exist but heuristics have prevented its implementation. Such a prospect is logically possible but unlikely; such individual have had plenty of opportunities to fulfill such a goal. Other preferences are prohibited due to their nonsensical nature, such as a preference to not fulfill your own preferences. The fact that we are system running on similar hardware and designed following similar processes point towards a certain convergence of morality. After all, you cannot install a x64 program in a x86 architecture. The common source of our preferences must surely account for similar preferences. Also, we live in convergent cultures, that further shape common preferences.

Despite this convergence, the prospect is far from ideal to the agent trying to extract the CEV of a set of individuals. Contradictory preferences will arise, and a compromise position between those must be adopted by the FAI in order for everybody to have their fair share.

One of the most elusive problems regarding CEV is when referring to preferences which reference other people preferences. Religious persons may want their peers to become religious. Rationalist would like their community to evolve toward Bayesian reasoning. And neither of them has a right over other people's preferences.

Reread last sentence. Curious, isn't it? Doesn't it strike you as a moral statement? And thus, was I to model my own preferences, one would take that form. At the very least, I would defend my own preferences, and then defend other people preferences as an instrumental way to defend my own preferences. This looks like a common thread in mankind.

What would happen, however, if one really wanted to change some other people preferences while other people are more flexible over their preferences? A FAI merely trying to optimize using a weighted mean could give preference to this high utility associated preference, effectively stomping other people's preferences. This is undesired behavior.

Trying to come up with a good algorhytm to fix this problem is, as with individual CEV, currently over my capabilities. And unlike with individual CEV, I do not dare to suggest an informal fix.

##Conclusion

We have divided the CEV problem in two parts: individual CEV extraction and synthesis of individual CEVs. An informal method of individual CEV extraction from a set of diverse order preferences has been proposed, though its feasibility and plausibility remain uncertain. The individual CEV extraction itself depends on a generalization of scientific induction, which is way beyond the current capabilities of machines. Synthesis of individual CEVs is a chaotic mess, and the lack of an angle of approach is evident.

Comment and discuss. Every deserved (an underserved) word is welcome.
