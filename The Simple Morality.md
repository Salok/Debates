#The Simple Morality

We constantly obsess over the nature of morality. We pretty much nail morality when working on object level, as evidenced by the sheer amount of moral theories who end up reaching the same conclusions, yet our meta-theories, our search for the source of morality, diverge wildly, suggesting most of us believe in a wrong explanation. If we want to program a FAI we may better amend our notion of morality before we end with a clip optimizer eating our face. The main reason to write this is to expose my own ideas to myself, and see whether any of them are valuable, so that I can rebuild my beliefs upon them. And, of course, to find the flaws and annihilate them, cleansing my mind under the fire of reason. Ultimately, my goal in mind is an analysis of the plausibility of the CEV concept.

The first grave mistake people make when talking about morality is not specifying a frame from which to discuss morality. There is no intrinsic property called rightness in the universe, so no action can be "right" or "wrong" if there is no agent which reasons about such properties. Now, this suggests that morality depends upon the preferences of the agent. I'll go one step further and say that what makes us talk about morality are actually our preferences.

Philosophers through history have made a sharp distinction between what we want and what we want to want. They have called the former thing our desires and the second thing moral compass or similar. I find such a division disturbing and unnatural. We do not need two buckets for those two things. Both of them are preferences, which in ultimate instance determine our acts. We need a distinction however between our true preferences, and our stated preferences, which is our model of what our preferences are.

What is a preference? This question seems resolved when we picture rational agents as optimization processes in the physical universe. Surely there must be a utility function attached to each agent, that effectively determines its actions. By preferences (and thus morality) I do refer to the formal specification of our utility function.

Let me delve in this idea from other point of view. Morality, whatever mental process it is, must be causally connected to our actions. So it seems like we must be able to infer morality, or at least a great part of it, observing the behavior of humankind. This is a highly controversial claim; however, it seems obvious to me in hindsight. After all, if morality was something causally disconnected from the individual, why let it control our actions?

Now that I have exposed my reasons to believe why extracting morality may be empirically possible, I proceed to investigate the obstacles we encounter when trying to realize this process.

##The problem of modeling morality

One apparently unsurmountable difficulty when dealing with our preferences is our lack of introspection. We do not seem able to pinpoint the exact specification that evolution forged into us; we are unable to read our own code. We do know some algorithm must explain our behavior, but we are still too young to comprehend it. Our experience with morality is akin to a black box, from which certain imperatives arise. So we have to rely on induction, and so will have a FAI implementing individual CEV, to try guessing the most simple set of rules that govern a subject's morality. This task has proven to be daunting due to several reasons, related to the inherent difficulty of modeling a real world process and to the content of our preferences.

When modeling preferences, we must rely on a machine capable of doing scientific induction. It will have to update a reasonable prior with its observations. A possible prior is the generally accepted [Kolgomorov simplicity](https://en.wikipedia.org/wiki/Minimum_description_length) epistemic axiom. The update will have to be realized according to a technique yet to be developed. The problem is akin to the production of software capable of performing fully fledged science. Furthermore, when updating, special care will have to be payed to screen off the dissonance between preferences and actions introduced by heuristics. The FAI will observe in subjects a behavior which does not reflect our exact preferences, but one systematically corrupted by the various biases that affect the human mind.

Other sensible problem would arise from the fact that preferences seem to change in relation to information available to us. Surely, the FAI would have to use its tremendous databases when extracting the CEV of an individual. A cheap way out of this is to assume that a preference subject to change in light of new information was actually an instrumental preference, being the final preference something other. Now, there are several problems with this solution, namely the difficulty in differentiating between instrumental and final values. This seems specially problematic when regarding values that are both final and instrumental to other values. For example, humans seem to regard life as a final value, but life is also a strong convergent instrumental value.

Regarding content, we seem to have contradictory preferences. There is a precise mathematical description of a rational set of preferences, [VNM rationality](https://en.wikipedia.org/wiki/Von_Neumann%E2%80%93Morgenstern_utility_theorem), and several psychological studies conclude that ours do not satisfy the required axioms. For example, we will accept a bet previously rejected when framed in a slightly different way. This could be consequence of our inability to fully compute our utility function (in other words, due to the effect of heuristics). Are truly contradictory preferences logically possible or just the apparent product of heuristics? I have trouble picturing an agent with contradictory preferences. However, such a thing seems possible if heuristics mediate between our stated preferences and our actions. Let me explain myself.

We could imagine contradictory preferences as two statements which cannot be fulfilled at the same time. Two preferences such as: "I like apples" and "I don't like apples". But this kind of contradiction can only exist at the indirect specification level. At object level, one will prevail. Either the agent will eat apples or he won't. The inconsistency is in the map, not in the territory.

Another difficulty when trying to model preferences relates to meta-preferences. We can have preferences of second order, which refer to which preferences we would like to have. For example, a drug addict may have a first order preference to take drugs and a second order preference not to do so. He could even have third order preferences and so forth. This complicates the analysis.

Other problem to take into account is that preferences evolve with time. Internally, as meta-preferences are realized they change the content of lesser order preferences. This change is gradual as the computation and implementation of preferences is subject to the imperfect reality of our inefficient bodies. Furthermore, for a meta preference to take effect its associated utility must surpass the utility provided by the lesser order preferences to be changed. Returning to the drug addict example, it may be impossible for him to ever leave drugs if the utility received from taking the drug has already surpassed the utility of changing his base preference. It is important to realize that the existence of meta preferences is a direct consequence of our urge driven behavior, that is unable to maximize its own meta-preferences. If somehow we could modify our preferences with little to no delay, every item in the preference set would be reduced to object level. However, we are far apart from the ideal of an agent, and this is not the case.

Our preferences can change due to extern stimuli as well. We may read a book and be inspired by its charismatic protagonist, changing our future reactions. This implies that a FAI implementing individual CEV must either make a model of how the subject preferences will evolve over time in his daily life, or periodically reevaluate his model of preferences of the subject. A compromise between both options seems like a good *a priori* template solution; the FAI will be able to rely in the evolving model between each reevaluation, while each update will make sure the model does not depart too much from reality.

Regarding the problems arising from meta preferences, I dare to suggest a method for extracting a VNM rational set of preferences from a collection of various orders preferences. The FAI would rely on scientific induction and Bayesian prior reasoning to deduce a reasonable set of preferences. Those will be arranged accorded to order, and then every higher order preference will be applied to optimize the base preferences until we are left with only coherent base preferences. In other words, we will first look the highest order preferences, say order n, and optimize n-1 order preferences according to those preferences. The n-1 optimized preferences will then be used to optimize n-2 preferences, and so forth, until first order preferences are reached.

I have been asked how could we infer higher order preferences from mere observation. My mind turns again to scientific induction and Occam prior reasoning. We could observe how a subject changes its preferences over time, and infer a second-order preference, instead of postulating several conditional or partial preferences. For example, if we observe that a subject gradually starts implementing preferences with content such as doing exercise and eating healthy, we could infer the existence of a second order preference "I want preferences that lead me to a healthier life". We can note that those higher order preferences are only relevant while they are not fully implemented in base level. Once our subject has implemented every first-order preference on health, the second order preference becomes irrelevant in the creation of new preferences, so it becomes irrelevant. It would then become a first order preference, absorbing every preference that it could have generated while being second order. So "I want to do exercise" and "I want to eat healthy" are grouped under the general preference "I want to do things that lead me to a healthier life" while "I want preferences that lead me to a healthier life" would become redundant. This is fully consistent with Kolgomorov simplicity.

It is unclear whether inconsistencies could arise from the optimization process, though it would seem like an inconsistent system is symptom of a poor construction of the initial set of the rules abstracting the utility function of the subject, as with our previous example about the apples. Another problem arises when we allow appliance of induction over preferences of increasing order. There is a very good chance of us having a meta-preference stating "I want to preserve my first order preferences". Similarly, a third order preference stating "I want to preserve my second order preferences" may be possible, and so forth. A FAI observing those preferences may conclude by induction a infinite order preference of preservation of preferences. The method would be rendered useless then. Note however that the example provided would make for an inconsistent set of preferences may there be another meta preference; this other hypothetical meta-preference would require changing a base preference to be optimized, but that contradicts the preservation preference. We are tempted to conclude that such an infinite order preference would be fruit again of a stupid epistemic system, but it is difficult to say whether other similar in structure preferences are possible.

Revising my beliefs about preferences, I have noticed I missed to take into account hyperbolic discount in my model of preferences. For those unfamiliar with the term, I am referring to the effect through which we tend to overvalue immediate consequences of our actions versus long term consequences. Thus, the pleasure of eating cake right now is weighted more than the benefit of healthiness that comes from not eating the cake. I claim hyperbolic discount is one of the biases the FAI would have to screen off to obtain our true preferences. It is unclear how such a process may be done. Full human brain simulation seems like would offer the possibility of being able to guess what an agent would actually prefer if he had time and power to carefully ponder its options. However, this is unfeasible with current technology, and I vaguely suspect we should aim for low computation power designs of the seed AI which will bootstrap itself to a mature FAI. In addition it makes the scary prospect of mind crime a serious possibility.

##Extrapolating morality from a set of individuals

So far we have examined how a morality may be extracted from an individual. Now we turn our attention to the far more difficult problem of extrapolating the CEV of a community.

Our previous reflections on preferences suggest a certain arbitrariness surrounding the contents of those. We have to realize however that there are some constraints, the most important of which is of evolutionary nature. There are certain preferences inconsistent with the mere existence of the individual. For example, an individual wanting to terminate its own life will soon cease to exist. It may be possible to admit that such a preference exist but heuristics have prevented its implementation. Such a prospect is logically possible but unlikely; such individual have had plenty of opportunities to fulfill such a goal. Other preferences are prohibited due to their nonsensical nature, such as a preference to not fulfill your own preferences. The fact that we are system running on similar hardware and designed following similar processes point towards a certain convergence of morality. After all, you cannot install a x64 program in a x86 architecture. The common source of our preferences must surely account for similar preferences. Also, we live in convergent cultures, that further shape common preferences.

Despite this convergence, the prospect is far from ideal to the agent trying to extract the CEV of a set of individuals. Contradictory preferences will arise, and a compromise position between those must be adopted by the FAI in order for everybody to have their fair share.

One of the most elusive problems regarding CEV is when referring to preferences which reference other people preferences. Religious persons may want their peers to become religious. Rationalists would like their community to evolve toward Bayesian reasoning. And neither of them has a right over other people's preferences.

Reread last sentence. Curious, isn't it? Doesn't it strike you as a moral statement? And thus, was I to model my own preferences, one would take that form. At the very least, I would defend my own preferences, and then defend other people preferences as an instrumental way to defend my own preferences. This looks like a common trait in mankind.

What would happen, however, if one really wanted to change some other people preferences while other people are more flexible over their preferences? A FAI merely trying to optimize using a weighted mean could give preference to this high utility associated preference, effectively stomping other people's preferences. This is undesired behavior.

Trying to come up with a good algorithm to fix this problem is, as with individual CEV, currently over my capabilities. However, it may be possible to specify mankind as a whole agent, and so the method of extracting preferences discussed before would apply.

##Conclusion

We have divided the CEV problem in two parts: individual CEV extraction and synthesis of individual CEVs. An informal method of individual CEV extraction from a set of diverse order preferences has been proposed, though its feasibility and plausibility remain uncertain. The individual CEV extraction itself depends on a generalization of scientific induction, which is way beyond the current capabilities of machines. Synthesis of individual CEVs is a chaotic mess, and the lack of an angle of approach is evident.

It seems likely that research toward extracting preferences of arbitrary agent would have to pass through decision theory, namely the definition of a useful notion of agent. We might reasonably believe that humans cannot be thought of as agents with goals, but instead processes driven entirely by urges. I suspect that even if this was the case we could still successfully extract consistent preferences from their behavior using the method suggested above.

Comment and discuss. Every deserved (an underserved) word is welcome.

##Key ideas
* Humans are optimization processes.
* Optimization processes can be characterized as agents with associated VNM preferences.
  * What constitues an optimization process?
    * What does NOT constitue an optimization process?
  * What does it mean to be characterized by a VNM function?
    * It means that we can deduce its behaviour from the UF.
      * We need to take into account biases, actual capabilities and lack of introspection.
      * Two kind of limitations: action limitations (does not change how the UF dictates behaviour, but restrains possible courses of action) and UF distortions (changes the UF implementation, leads to irrational behaviour).
    * Maximizing the associated UF maximizes the agent values.
      * Those two things seem equivalent to affirming that actual preferences correspond with behavioral preferences in some          sense.
  * What constitues an agent?
    * Preferences + Model of the world + Decisions?
    * How can we separate an agent from the environment?
    * Are agents a special kind of optimization process?
* The preferences that model an optimization process can be inferred using natural induction.
  * Stated preferences VS Behavioral preferences VS Actual preferences.
  * Screening off biases and heuristics.
* Open question: CEV of humankind.
  * Possible solution: Regard humankind as an optimization process.
  * Other possible solution: If individual values are sufficiently alike, extrapolate the individual UF using magic.
